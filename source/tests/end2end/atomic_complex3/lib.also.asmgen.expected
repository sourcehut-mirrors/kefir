.att_syntax
.section .note.GNU-stack,"",%progbits

.global add_f32
.type add_f32, @function
.global add_f64
.type add_f64, @function
.global divide_f32
.type divide_f32, @function
.global divide_f64
.type divide_f64, @function
.global multiply_f32
.type multiply_f32, @function
.global multiply_f64
.type multiply_f64, @function
.global subtract_f32
.type subtract_f32, @function
.global subtract_f64
.type subtract_f64, @function

.section .text
__kefir_text_section_begin:
add_f32:
__kefir_text_func_add_f32_begin:
    push %rbp
    mov %rsp, %rbp
    sub $96, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -96(%rbp)
    movq -96(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_add_f32_label2:
    movq %r10, -88(%rbp)
    mov %r10, %rsi
    lea -96(%rbp), %rdx
    mov $8, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -88(%rbp), %r10
    movd -96(%rbp), %xmm0
    movd -92(%rbp), %xmm1
    addss -72(%rbp), %xmm0
    addss -68(%rbp), %xmm1
    movd %xmm0, -80(%rbp)
    movd %xmm1, -76(%rbp)
    movq %r10, -32(%rbp)
    movq -96(%rbp), %rax
    movq %rax, -88(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $8, %rdi
    lea -88(%rbp), %rdx
    leaq -80(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_add_f32_label4
    stmxcsr -96(%rbp)
    mov -96(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -96(%rbp)
    orl %eax, -96(%rbp)
    ldmxcsr -96(%rbp)
    movq -80(%rbp), %xmm0
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_add_f32_label4:
    fnclex
    jmp _kefir_func_add_f32_label2
__kefir_text_func_add_f32_end:

add_f64:
__kefir_text_func_add_f64_begin:
    push %rbp
    mov %rsp, %rbp
    sub $128, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -128(%rbp)
    movq %xmm1, -120(%rbp)
    movq -128(%rbp), %xmm0
    movq %xmm0, -24(%rbp)
    movq -120(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -24(%rbp), %xmm0
    movq %xmm0, -80(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_add_f64_label2:
    movq %r10, -112(%rbp)
    mov %r10, %rsi
    lea -128(%rbp), %rdx
    mov $16, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -112(%rbp), %r10
    movq -128(%rbp), %xmm0
    movq -120(%rbp), %xmm1
    addsd -80(%rbp), %xmm0
    addsd -72(%rbp), %xmm1
    movq %xmm0, -96(%rbp)
    movq %xmm1, -88(%rbp)
    movq %r10, -32(%rbp)
    movdqu -128(%rbp), %xmm0
    movdqu %xmm0, -112(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $16, %rdi
    lea -112(%rbp), %rdx
    leaq -96(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_add_f64_label4
    stmxcsr -128(%rbp)
    mov -128(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -128(%rbp)
    orl %eax, -128(%rbp)
    ldmxcsr -128(%rbp)
    movq -96(%rbp), %xmm0
    movq -88(%rbp), %xmm1
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_add_f64_label4:
    fnclex
    jmp _kefir_func_add_f64_label2
__kefir_text_func_add_f64_end:

divide_f32:
__kefir_text_func_divide_f32_begin:
    push %rbp
    mov %rsp, %rbp
    sub $96, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -96(%rbp)
    movq -96(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_divide_f32_label2:
    movq %r10, -88(%rbp)
    mov %r10, %rsi
    lea -96(%rbp), %rdx
    mov $8, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -88(%rbp), %r10
    movq -96(%rbp), %xmm0
    movq -72(%rbp), %xmm1
    cvtps2pd %xmm1, %xmm1
    cvtps2pd %xmm0, %xmm0
    movaps %xmm1, %xmm2
    movaps %xmm1, %xmm3
    unpcklpd %xmm1, %xmm2
    mulpd %xmm0, %xmm2
    shufpd $1, %xmm0, %xmm0
    unpckhpd %xmm1, %xmm3
    mulpd %xmm1, %xmm1
    mulpd %xmm3, %xmm0
    movaps %xmm1, %xmm3
    xorps __kefir_constant_complex_float32_div(%rip), %xmm0
    shufpd $1, %xmm1, %xmm3
    addpd %xmm0, %xmm2
    addpd %xmm3, %xmm1
    divpd %xmm1, %xmm2
    cvtpd2ps %xmm2, %xmm0
    movq %xmm0, -80(%rbp)
    movq %r10, -32(%rbp)
    movq -96(%rbp), %rax
    movq %rax, -88(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $8, %rdi
    lea -88(%rbp), %rdx
    leaq -80(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_divide_f32_label4
    stmxcsr -96(%rbp)
    mov -96(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -96(%rbp)
    orl %eax, -96(%rbp)
    ldmxcsr -96(%rbp)
    movq -80(%rbp), %xmm0
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_divide_f32_label4:
    fnclex
    jmp _kefir_func_divide_f32_label2
__kefir_text_func_divide_f32_end:

divide_f64:
__kefir_text_func_divide_f64_begin:
    push %rbp
    mov %rsp, %rbp
    sub $128, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -128(%rbp)
    movq %xmm1, -120(%rbp)
    movq -128(%rbp), %xmm0
    movq %xmm0, -24(%rbp)
    movq -120(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -24(%rbp), %xmm0
    movq %xmm0, -80(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_divide_f64_label2:
    movq %r10, -128(%rbp)
    mov %r10, %rsi
    lea -112(%rbp), %rdx
    mov $16, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -128(%rbp), %r10
    fld1
    fldl -80(%rbp)
    fld %st(0)
    fmul %st(1)
    fldl -72(%rbp)
    fld %st(0)
    fmul %st(1)
    faddp %st(2)
    fxch %st(1)
    fdivrp %st(3)
    fldl -112(%rbp)
    fld %st(0)
    fmul %st(3)
    fxch %st(1)
    fmul %st(2)
    fldl -104(%rbp)
    fld %st(0)
    fmulp %st(4)
    fxch %st(3)
    faddp %st(2)
    fxch %st(1)
    fmul %st(4)
    fstpl -96(%rbp)
    fxch %st(2)
    fmulp %st(1)
    fsubp
    fmulp
    fstpl -88(%rbp)
    movq %r10, -32(%rbp)
    movdqu -112(%rbp), %xmm0
    movdqu %xmm0, -128(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $16, %rdi
    lea -128(%rbp), %rdx
    leaq -96(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_divide_f64_label4
    stmxcsr -128(%rbp)
    mov -128(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -128(%rbp)
    orl %eax, -128(%rbp)
    ldmxcsr -128(%rbp)
    movq -96(%rbp), %xmm0
    movq -88(%rbp), %xmm1
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_divide_f64_label4:
    fnclex
    jmp _kefir_func_divide_f64_label2
__kefir_text_func_divide_f64_end:

multiply_f32:
__kefir_text_func_multiply_f32_begin:
    push %rbp
    mov %rsp, %rbp
    sub $96, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -96(%rbp)
    movq -96(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_multiply_f32_label2:
    movq %r10, -88(%rbp)
    mov %r10, %rsi
    lea -96(%rbp), %rdx
    mov $8, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -88(%rbp), %r10
    movq -96(%rbp), %xmm0
    movq -72(%rbp), %xmm1
    movaps %xmm1, %xmm2
    shufps $160, %xmm1, %xmm2
    mulps %xmm0, %xmm2
    xorps __kefir_constant_complex_float32_mul(%rip), %xmm0
    shufps $177, %xmm0, %xmm0
    shufps $245, %xmm1, %xmm1
    mulps %xmm1, %xmm0
    addps %xmm2, %xmm0
    movq %xmm0, -80(%rbp)
    movq %r10, -32(%rbp)
    movq -96(%rbp), %rax
    movq %rax, -88(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $8, %rdi
    lea -88(%rbp), %rdx
    leaq -80(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_multiply_f32_label4
    stmxcsr -96(%rbp)
    mov -96(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -96(%rbp)
    orl %eax, -96(%rbp)
    ldmxcsr -96(%rbp)
    movq -80(%rbp), %xmm0
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_multiply_f32_label4:
    fnclex
    jmp _kefir_func_multiply_f32_label2
__kefir_text_func_multiply_f32_end:

multiply_f64:
__kefir_text_func_multiply_f64_begin:
    push %rbp
    mov %rsp, %rbp
    sub $128, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -128(%rbp)
    movq %xmm1, -120(%rbp)
    movq -128(%rbp), %xmm0
    movq %xmm0, -24(%rbp)
    movq -120(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -24(%rbp), %xmm0
    movq %xmm0, -80(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_multiply_f64_label2:
    movq %r10, -112(%rbp)
    mov %r10, %rsi
    lea -128(%rbp), %rdx
    mov $16, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -112(%rbp), %r10
    movq -128(%rbp), %xmm0
    movq -120(%rbp), %xmm1
    movq -80(%rbp), %xmm2
    movq -72(%rbp), %xmm3
    unpcklpd %xmm1, %xmm0
    movaps %xmm0, %xmm1
    xorps __kefir_constant_complex_float64_mul(%rip), %xmm1
    shufpd $1, %xmm1, %xmm1
    unpcklpd %xmm2, %xmm2
    unpcklpd %xmm3, %xmm3
    mulpd %xmm0, %xmm2
    mulpd %xmm1, %xmm3
    addpd %xmm2, %xmm3
    movaps %xmm3, %xmm0
    movaps %xmm3, %xmm1
    unpckhpd %xmm3, %xmm0
    movq %xmm1, -96(%rbp)
    movq %xmm0, -88(%rbp)
    movq %r10, -32(%rbp)
    movdqu -128(%rbp), %xmm0
    movdqu %xmm0, -112(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $16, %rdi
    lea -112(%rbp), %rdx
    leaq -96(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_multiply_f64_label4
    stmxcsr -128(%rbp)
    mov -128(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -128(%rbp)
    orl %eax, -128(%rbp)
    ldmxcsr -128(%rbp)
    movq -96(%rbp), %xmm0
    movq -88(%rbp), %xmm1
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_multiply_f64_label4:
    fnclex
    jmp _kefir_func_multiply_f64_label2
__kefir_text_func_multiply_f64_end:

subtract_f32:
__kefir_text_func_subtract_f32_begin:
    push %rbp
    mov %rsp, %rbp
    sub $96, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -96(%rbp)
    movq -96(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_subtract_f32_label2:
    movq %r10, -88(%rbp)
    mov %r10, %rsi
    lea -96(%rbp), %rdx
    mov $8, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -88(%rbp), %r10
    movd -96(%rbp), %xmm0
    movd -92(%rbp), %xmm1
    subss -72(%rbp), %xmm0
    subss -68(%rbp), %xmm1
    movd %xmm0, -80(%rbp)
    movd %xmm1, -76(%rbp)
    movq %r10, -32(%rbp)
    movq -96(%rbp), %rax
    movq %rax, -88(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $8, %rdi
    lea -88(%rbp), %rdx
    leaq -80(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_subtract_f32_label4
    stmxcsr -96(%rbp)
    mov -96(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -96(%rbp)
    orl %eax, -96(%rbp)
    ldmxcsr -96(%rbp)
    movq -80(%rbp), %xmm0
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_subtract_f32_label4:
    fnclex
    jmp _kefir_func_subtract_f32_label2
__kefir_text_func_subtract_f32_end:

subtract_f64:
__kefir_text_func_subtract_f64_begin:
    push %rbp
    mov %rsp, %rbp
    sub $128, %rsp
    stmxcsr -8(%rbp)
    mov %rdi, %r10
    movq %xmm0, -128(%rbp)
    movq %xmm1, -120(%rbp)
    movq -128(%rbp), %xmm0
    movq %xmm0, -24(%rbp)
    movq -120(%rbp), %xmm0
    movq %xmm0, -16(%rbp)
    movq -24(%rbp), %xmm0
    movq %xmm0, -80(%rbp)
    movq -16(%rbp), %xmm0
    movq %xmm0, -72(%rbp)
    fnstenv -64(%rbp)
    stmxcsr -36(%rbp)
    fnclex
_kefir_func_subtract_f64_label2:
    movq %r10, -112(%rbp)
    mov %r10, %rsi
    lea -128(%rbp), %rdx
    mov $16, %rdi
    mov $5, %rcx
    call __atomic_load@PLT
    movq -112(%rbp), %r10
    movq -128(%rbp), %xmm0
    movq -120(%rbp), %xmm1
    subsd -80(%rbp), %xmm0
    subsd -72(%rbp), %xmm1
    movq %xmm0, -96(%rbp)
    movq %xmm1, -88(%rbp)
    movq %r10, -32(%rbp)
    movdqu -128(%rbp), %xmm0
    movdqu %xmm0, -112(%rbp)
    mov %r10, %rsi
    mov $5, %r8
    mov $5, %r9
    mov $16, %rdi
    lea -112(%rbp), %rdx
    leaq -96(%rbp), %rax
    mov %rax, %rcx
    call __atomic_compare_exchange@PLT
    movq -32(%rbp), %r10
    test %al, %al
    jz _kefir_func_subtract_f64_label4
    stmxcsr -128(%rbp)
    mov -128(%rbp), %rdx
    fnstsw %ax
    or %edx, %eax
    and $63, %eax
    fldenv -64(%rbp)
    ldmxcsr -36(%rbp)
    stmxcsr -128(%rbp)
    orl %eax, -128(%rbp)
    ldmxcsr -128(%rbp)
    movq -96(%rbp), %xmm0
    movq -88(%rbp), %xmm1
    ldmxcsr -8(%rbp)
    lea (%rbp), %rsp
    pop %rbp
    ret
_kefir_func_subtract_f64_label4:
    fnclex
    jmp _kefir_func_subtract_f64_label2
__kefir_text_func_subtract_f64_end:

__kefir_text_section_end:

.section .rodata
    .align 16
__kefir_constant_complex_float32_mul:
    .long 0
    .long 2147483648
    .long 0
    .long 2147483648
    .align 16
__kefir_constant_complex_float32_div:
    .long 0
    .long 0
    .long 0
    .long 2147483648
    .align 16
__kefir_constant_complex_float64_mul:
    .long 0
    .long 0
    .long 0
    .long 2147483648
